# -*- coding: utf-8 -*-
"""fer_vgg16_pytorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bv29jCDQJIm4G1MIcKNTFCnl0norYC_d
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torchvision.models as models
from torch.utils.data import DataLoader
from google.colab import drive
import os
import shutil
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, accuracy_score, roc_curve, auc, precision_recall_curve, average_precision_score

# Mount Google Drive
drive.mount('/content/drive')

# Source and destination paths
source_path = '/content/drive/MyDrive/Research_Vaish/DATASET'
destination_path = '/content/dataset'

# Remove existing destination directory
if os.path.exists(destination_path):
    for filename in os.listdir(destination_path):
        file_path = os.path.join(destination_path, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.unlink(file_path)
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
        except Exception as e:
            print('Failed to delete %s. Reason: %s' % (file_path, e))

# Create directory and copy dataset
os.makedirs(destination_path, exist_ok=True)
shutil.copytree(source_path, destination_path, dirs_exist_ok=True)

data_dir = '/content/dataset'

# Define transformations
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

batch_size = 32

# Load dataset
train_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'train'), transform=transform)
val_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'test'), transform=transform)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)

print(f"Number of images in train set: {len(train_dataset)}")#12271
print(f"Number of images in test set: {len(val_dataset)}")#3068

# Load VGG16 model
model = models.vgg16(weights=models.VGG16_Weights.DEFAULT)  # Updated
num_features = model.classifier[6].in_features
model.classifier[6] = nn.Linear(num_features, len(train_dataset.classes))

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = model.to(device)

# Training function
def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.train()
    print(f"Using device: {device}")

    for epoch in range(epochs):
        running_loss = 0.0
        correct = 0
        total = 0
        batch_count = 0

        model.train()
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            batch_count += 1

            if batch_count % max(1, int((len(train_dataset) / batch_size) // 6)) == 0:
                print(f"Epoch {epoch+1}, Batch {batch_count}, Loss: {running_loss / batch_count:.4f}, Accuracy: {100 * correct / total:.2f}%")
            torch.cuda.empty_cache()  # Clear GPU memory

        train_loss = running_loss / len(train_loader)
        train_acc = 100 * correct / total

        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        with torch.no_grad():
            for val_images, val_labels in val_loader:
                val_images, val_labels = val_images.to(device), val_labels.to(device)
                val_outputs = model(val_images)
                val_loss += criterion(val_outputs, val_labels).item()
                _, val_predicted = val_outputs.max(1)
                val_total += val_labels.size(0)
                val_correct += val_predicted.eq(val_labels).sum().item()

        val_loss /= len(val_loader)
        val_acc = 100 * val_correct / val_total

        print(f"Epoch {epoch+1} Summary - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")

    return model

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)
# Train the model
model = train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20)

# Save the model
torch.save(model.state_dict(), '/content/drive/MyDrive/vgg16_facial_expression.pth')

from sklearn.metrics import classification_report, accuracy_score, roc_curve, auc, precision_recall_curve, average_precision_score

# Evaluation function with PR Curve
def evaluate_model(model, val_loader):
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.eval()

    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            probs = torch.softmax(outputs, dim=1)  # Get class probabilities
            _, predicted = outputs.max(1)

            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    # Convert lists to numpy arrays
    all_labels = np.array(all_labels, dtype=int)
    all_probs = np.array(all_probs)

    print("Classification Report:")
    print(classification_report(all_labels, all_preds, target_names=val_dataset.classes))
    print("Accuracy:", accuracy_score(all_labels, all_preds))

    # One-hot encoding of labels for multi-class PR Curve
    all_labels_one_hot = np.eye(len(val_dataset.classes))[all_labels]

    # ROC Curve
    plt.figure(figsize=(8, 6))
    for i in range(len(val_dataset.classes)):
        fpr, tpr, _ = roc_curve(all_labels_one_hot[:, i], all_probs[:, i])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'Class {val_dataset.classes[i]} (AUC = {roc_auc:.2f})')

    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend()
    plt.show()

    # Precision-Recall Curve
    plt.figure(figsize=(8, 6))
    for i in range(len(val_dataset.classes)):
        precision, recall, _ = precision_recall_curve(all_labels_one_hot[:, i], all_probs[:, i])
        pr_auc = average_precision_score(all_labels_one_hot[:, i], all_probs[:, i])
        plt.plot(recall, precision, label=f'Class {val_dataset.classes[i]} (AP = {pr_auc:.2f})')

    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.legend()
    plt.show()

# Call the function after training
evaluate_model(model, val_loader)


# -*- coding: utf-8 -*-
"""ResNet_L1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UsttsjUlxkUt3Ws8QBT3wyN81shHzYUi
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torchvision.models as models
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import classification_report, accuracy_score, roc_curve, auc, precision_recall_curve, average_precision_score
import shutil

from google.colab import drive
drive.mount('/content/drive')

# Source and destination paths
source_path = '/content/drive/MyDrive/Research_Vaish/DATASET RAF-DB'
destination_path = '/content/dataset'

# Remove existing destination directory
if os.path.exists(destination_path):
    shutil.rmtree(destination_path)

# Create directory and copy dataset
os.makedirs(destination_path, exist_ok=True)
shutil.copytree(source_path, destination_path, dirs_exist_ok=True)

data_dir = destination_path

# Define transforms
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# Load dataset
batch_size = 32
train_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'train'), transform=transform)
val_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'test'), transform=transform)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Load ResNet50
model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, len(train_dataset.classes))
model = model.to(device)

# Loss function
criterion = nn.CrossEntropyLoss()
initial_lr = 1e-4
optimizer = optim.Adam(model.parameters(), lr=initial_lr)

# Trackers
train_losses, val_losses = [], []
train_accuracies, val_accuracies = [], []

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import LambdaLR
from torchvision import models
from tqdm.notebook import tqdm
import matplotlib.pyplot as plt
from google.colab import drive

# ==== Mount Google Drive ====
drive.mount('/content/drive')

# ==== Define Save Path ====
save_path = "/content/drive/MyDrive/Research_Vaishnavi/best"
os.makedirs(save_path, exist_ok=True)
save_file = os.path.join(save_path, "best_model_cbam_L1.pth")

# ==== CBAM Definition (Only for Last Layer) ====
class ChannelAttention(nn.Module):
    def __init__(self, in_planes, ratio=8):
        super(ChannelAttention, self).__init__()
        self.shared_MLP = nn.Sequential(
            nn.Linear(in_planes, in_planes // ratio),
            nn.ReLU(),
            nn.Linear(in_planes // ratio, in_planes)
        )
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=(2, 3), keepdim=False)
        max_out, _ = torch.max(x, dim=2, keepdim=False) # Fix: Apply max sequentially
        max_out, _ = torch.max(max_out, dim=2, keepdim=False) # Fix: Apply max sequentially
        avg_out = self.shared_MLP(avg_out)
        max_out = self.shared_MLP(max_out)
        out = avg_out + max_out
        return self.sigmoid(out).unsqueeze(2).unsqueeze(3)

class SpatialAttention(nn.Module):
    def __init__(self):
        super(SpatialAttention, self).__init__()
        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x_cat = torch.cat([avg_out, max_out], dim=1)
        x_out = self.conv(x_cat)
        return self.sigmoid(x_out)

class CBAM_LastLayer(nn.Module):
    def __init__(self, in_channels):
        super(CBAM_LastLayer, self).__init__()
        self.ca = ChannelAttention(in_channels)
        self.sa = SpatialAttention()

    def forward(self, x):
        x = x * self.ca(x)
        x = x * self.sa(x)
        return x

# ==== Modified ResNet50 with CBAM Only at Last Layer ====
class ResNet50_CBAM_LastLayer(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        base_model = models.resnet50(pretrained=True)
        self.features = nn.Sequential(*list(base_model.children())[:-2])  # up to conv5_x
        self.cbam = CBAM_LastLayer(2048)  # Apply CBAM on last conv output
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.flatten = nn.Flatten()
        self.fc = nn.Linear(2048, num_classes)

    def forward(self, x):
        x = self.features(x)
        x = self.cbam(x)
        x = self.pool(x)
        x = self.flatten(x)
        x = self.fc(x)
        return x

# ==== Instantiate Model ====
num_classes = 7  # change if needed
model = ResNet50_CBAM_LastLayer(num_classes=num_classes)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# ==== Optimizer and 2-Phase LR Scheduler ====
initial_lr = 1e-4
optimizer = optim.Adam(model.parameters(), lr=initial_lr)

def lr_schedule(epoch):
    if epoch < 50:
        return 1.0
    elif epoch < 60:
        return 0.5
    elif epoch < 70:
        return 0.25
    elif epoch < 80:
        return 0.125
    elif epoch < 90:
        return 0.0625
    else:
        return 0.03125

scheduler = LambdaLR(optimizer, lr_lambda=lr_schedule)
criterion = nn.CrossEntropyLoss()

# ==== Training & Validation Loop ====
train_losses, val_losses = [], []
train_accs, val_accs = [], []
best_acc = 0.0
num_epochs = 100

for epoch in range(num_epochs):
    print(f"\nEpoch {epoch+1}/{num_epochs}")

    # --- Training ---
    model.train()
    running_loss = 0.0
    running_corrects = 0
    for inputs, labels in tqdm(train_loader, desc="Training"):
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * inputs.size(0)
        running_corrects += torch.sum(preds == labels)

    epoch_loss = running_loss / len(train_dataset)
    epoch_acc = running_corrects.double() / len(train_dataset)
    train_losses.append(epoch_loss)
    train_accs.append(epoch_acc.item()) # Append accuracy to list
    print(f"Train Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}")

    # --- Validation ---
    model.eval()
    val_loss = 0.0
    val_corrects = 0
    with torch.no_grad():
        for inputs, labels in tqdm(val_loader, desc="Validation"):
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            loss = criterion(outputs, labels)
            val_loss += loss.item() * inputs.size(0)
            val_corrects += torch.sum(preds == labels)

    val_loss /= len(val_dataset)
    val_acc = val_corrects.double() / len(val_dataset)
    val_losses.append(val_loss)
    val_accs.append(val_acc.item()) # Append accuracy to list
    print(f"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}")

    # --- Scheduler Step ---
    scheduler.step()

    # --- Save Best Model ---
    if val_acc > best_acc:
        best_acc = val_acc
        torch.save({
            'epoch': epoch + 1,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_acc': val_acc.item(),
            'val_loss': val_loss
        }, save_file)
        print(f"âœ… Best model saved at epoch {epoch+1} with Val Acc: {val_acc:.4f}")

print("ðŸŽ‰ Training Completed!")

# Evaluation
def evaluate_model(model, val_loader):
    model.eval()
    all_preds, all_labels, all_probs = [], [], []

    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            probs = torch.softmax(outputs, dim=1)
            _, predicted = outputs.max(1)

            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    all_labels = np.array(all_labels)
    all_preds = np.array(all_preds)
    all_probs = np.array(all_probs)
    class_names = val_dataset.classes

    # Classification Report
    print("\nClassification Report:")
    print(classification_report(all_labels, all_preds, target_names=class_names))

    # ROC Curve
    one_hot = np.eye(len(class_names))[all_labels]
    plt.figure(figsize=(8, 6))
    for i in range(len(class_names)):
        fpr, tpr, _ = roc_curve(one_hot[:, i], all_probs[:, i])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'{class_names[i]} (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.title('ROC Curve')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend()
    plt.grid()
    plt.show()

    # PR Curve
    plt.figure(figsize=(8, 6))
    for i in range(len(class_names)):
        precision, recall, _ = precision_recall_curve(one_hot[:, i], all_probs[:, i])
        ap_score = average_precision_score(one_hot[:, i], all_probs[:, i])
        plt.plot(recall, precision, label=f'{class_names[i]} (AP = {ap_score:.2f})')
    plt.title('Precision-Recall Curve')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.legend()
    plt.grid()
    plt.show()

# Run evaluation
evaluate_model(model, val_loader)

# Loss/Accuracy Plots
epochs = range(1, len(train_losses) + 1)
plt.figure(figsize=(10, 4))
plt.plot(epochs, train_losses, label='Train Loss')
plt.plot(epochs, val_losses, label='Validation Loss')
plt.title('Loss per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid()
plt.show()

plt.figure(figsize=(10, 4))
plt.plot(epochs, train_accs, label='Train Accuracy') # Changed to use train_accs
plt.plot(epochs, val_accs, label='Validation Accuracy') # Changed to use val_accs
plt.title('Accuracy per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.legend()
plt.grid()
plt.show()


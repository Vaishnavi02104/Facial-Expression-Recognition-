# -*- coding: utf-8 -*-
"""ResNet_L4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VOUkY8dXJOAJ9rABDbIk7kAY7v1__tM_
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torchvision.models as models
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import classification_report, accuracy_score, roc_curve, auc, precision_recall_curve, average_precision_score
import shutil

# ===============================
# Google Drive Mount (Colab)
# ===============================
from google.colab import drive
drive.mount('/content/drive')

# Source and destination paths
source_path = '/content/drive/MyDrive/Research_Vaish/DATASET RAF-DB'
destination_path = '/content/dataset'

# Remove existing destination directory
if os.path.exists(destination_path):
    shutil.rmtree(destination_path)

# Create directory and copy dataset
os.makedirs(destination_path, exist_ok=True)
shutil.copytree(source_path, destination_path, dirs_exist_ok=True)

data_dir = destination_path

# Define transforms
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# Load dataset
batch_size = 32
train_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'train'), transform=transform)
val_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'test'), transform=transform)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Load ResNet50
model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, len(train_dataset.classes))
model = model.to(device)

# Loss function
criterion = nn.CrossEntropyLoss()
initial_lr = 1e-4
optimizer = optim.Adam(model.parameters(), lr=initial_lr)

# Trackers
train_losses, val_losses = [], []
train_accuracies, val_accuracies = [], []

from torch.optim.lr_scheduler import LambdaLR
from tqdm.notebook import tqdm

# ==== Define Save Path ====
save_path = "/content/drive/MyDrive/Research_Vaish"
os.makedirs(save_path, exist_ok=True)
save_file = os.path.join(save_path, "best_model_cbam_L4.pth")

# ===============================
# Define CBAM Module
# ===============================
class ChannelAttention(nn.Module):
    def __init__(self, in_planes, ratio=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        self.fc = nn.Sequential(
            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),
            nn.ReLU(),
            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)
        )
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc(self.avg_pool(x))
        max_out = self.fc(self.max_pool(x))
        out = avg_out + max_out
        return self.sigmoid(out)


class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        padding = 3 if kernel_size == 7 else 1
        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avg_out, max_out], dim=1)
        x = self.conv1(x)
        return self.sigmoid(x)


class CBAM(nn.Module):
    def __init__(self, in_planes, ratio=16, kernel_size=7):
        super(CBAM, self).__init__()
        self.ca = ChannelAttention(in_planes, ratio)
        self.sa = SpatialAttention(kernel_size)

    def forward(self, x):
        out = x * self.ca(x)
        out = out * self.sa(out)
        return out

# ===============================
# ResNet50 + CBAM in all 4 layers
# ===============================
class ResNet50_CBAM_All(nn.Module):
    def __init__(self, num_classes=7):
        super(ResNet50_CBAM_All, self).__init__()
        base_model = models.resnet50(pretrained=True)

        self.layer0 = nn.Sequential(
            base_model.conv1,
            base_model.bn1,
            base_model.relu,
            base_model.maxpool
        )
        self.layer1 = base_model.layer1
        self.layer2 = base_model.layer2
        self.layer3 = base_model.layer3
        self.layer4 = base_model.layer4

        # Add CBAM to ALL FOUR layers
        self.cbam1 = CBAM(256)
        self.cbam2 = CBAM(512)
        self.cbam3 = CBAM(1024)
        self.cbam4 = CBAM(2048)

        self.avgpool = base_model.avgpool
        self.fc = nn.Linear(2048, num_classes)

    def forward(self, x):
        x = self.layer0(x)

        x = self.layer1(x)
        x = self.cbam1(x)  # CBAM on layer1

        x = self.layer2(x)
        x = self.cbam2(x)  # CBAM on layer2

        x = self.layer3(x)
        x = self.cbam3(x)  # CBAM on layer3

        x = self.layer4(x)
        x = self.cbam4(x)  # CBAM on layer4

        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x

# ===============================
# Training & Evaluation Functions
# ===============================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = ResNet50_CBAM_All(num_classes=7).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

# LR schedule: 1e-4 for 50 epochs, then half per 10 epochs
from torch.optim import lr_scheduler
scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[50, 60, 70, 80, 90], gamma=0.5)

def train_one_epoch(model, loader, criterion, optimizer):
    model.train()
    running_loss, correct = 0.0, 0
    for inputs, labels in loader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * inputs.size(0)
        correct += (outputs.argmax(1) == labels).sum().item()
    return running_loss / len(loader.dataset), correct / len(loader.dataset)

def evaluate(model, loader, criterion):
    model.eval()
    running_loss, correct = 0.0, 0
    all_preds, all_labels = [], []
    with torch.no_grad():
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            running_loss += loss.item() * inputs.size(0)
            correct += (outputs.argmax(1) == labels).sum().item()
            all_preds.extend(outputs.argmax(1).cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    return running_loss / len(loader.dataset), correct / len(loader.dataset), all_preds, all_labels

# ===============================
# Training Loop with Best Checkpoint
# ===============================
best_val_acc = 0.0
train_losses, val_losses, train_accs, val_accs = [], [], [], []

save_file = "/content/drive/MyDrive/Research_Vaish/resnet50_cbam_L4.pth"

for epoch in range(100):
    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)
    val_loss, val_acc, preds, labels = evaluate(model, val_loader, criterion)

    scheduler.step()

    train_losses.append(train_loss)
    val_losses.append(val_loss)
    train_accs.append(train_acc)
    val_accs.append(val_acc)

    print(f"Epoch {epoch+1}/100 | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}")

    # Save best model
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), save_file)
        print(f"âœ… Saved Best Model (Val Acc = {val_acc:.4f})")

# ========================
# Classification Report, Curves, ROC & PR
# ========================
from sklearn.metrics import classification_report, roc_curve, auc, precision_recall_curve
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt
import numpy as np

# Reload best model
model.load_state_dict(torch.load(save_file))
model.eval()

all_preds = []
all_targets = []
all_probs = []

with torch.no_grad():
    for inputs, labels in val_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        probs = torch.softmax(outputs, dim=1)  # prediction probabilities
        preds = torch.argmax(probs, dim=1)

        all_preds.append(preds.cpu().numpy())
        all_targets.append(labels.cpu().numpy())
        all_probs.append(probs.cpu().numpy())

# Convert lists to arrays
all_preds = np.concatenate(all_preds, axis=0)
all_targets = np.concatenate(all_targets, axis=0)
all_probs = np.concatenate(all_probs, axis=0)

# ========================
# Classification Report
# ========================
print("\nClassification Report (Validation/Test Set):")
print(classification_report(all_targets, all_preds))

# ========================
# Loss & Accuracy Curves
# ========================
plt.plot(train_losses, label="Train Loss")
plt.plot(val_losses, label="Val Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.title("Loss Curve")
plt.show()

plt.plot(train_accs, label="Train Acc")
plt.plot(val_accs, label="Val Acc")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Accuracy Curve")
plt.show()

# ========================
# ROC & PR Curves (Multi-class)
# ========================
n_classes = all_probs.shape[1]
targets_bin = label_binarize(all_targets, classes=list(range(n_classes)))

# ROC Curve for each class
plt.figure(figsize=(8,6))
for i in range(n_classes):
    fpr, tpr, _ = roc_curve(targets_bin[:, i], all_probs[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f"Class {i} (AUC={roc_auc:.2f})")

plt.plot([0, 1], [0, 1], "k--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Multi-class ROC Curve")
plt.legend()
plt.show()

# PR Curve for each class
plt.figure(figsize=(8,6))
for i in range(n_classes):
    precision, recall, _ = precision_recall_curve(targets_bin[:, i], all_probs[:, i])
    plt.plot(recall, precision, label=f"Class {i}")

plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Multi-class Precision-Recall Curve")
plt.legend()
plt.show()


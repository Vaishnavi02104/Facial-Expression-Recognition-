# -*- coding: utf-8 -*-
"""ResNet_L3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LOPerPu_rqeXCjw5XF1gClITOJwewmPI
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torchvision.models as models
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import classification_report, accuracy_score, roc_curve, auc, precision_recall_curve, average_precision_score
import shutil

# ===============================
# Google Drive Mount (Colab)
# ===============================
from google.colab import drive
drive.mount('/content/drive')

# Source and destination paths
source_path = '/content/drive/MyDrive/Research_Vaish/DATASET RAF-DB'
destination_path = '/content/dataset'

# Remove existing destination directory
if os.path.exists(destination_path):
    shutil.rmtree(destination_path)

# Create directory and copy dataset
os.makedirs(destination_path, exist_ok=True)
shutil.copytree(source_path, destination_path, dirs_exist_ok=True)

data_dir = destination_path

# Define transforms
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# Load dataset
batch_size = 32
train_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'train'), transform=transform)
val_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'test'), transform=transform)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Load ResNet50
model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, len(train_dataset.classes))
model = model.to(device)

# Loss function
criterion = nn.CrossEntropyLoss()
initial_lr = 1e-4
optimizer = optim.Adam(model.parameters(), lr=initial_lr)

# Trackers
train_losses, val_losses = [], []
train_accuracies, val_accuracies = [], []

from torch.optim.lr_scheduler import LambdaLR
from tqdm.notebook import tqdm

# ==== Define Save Path ====
save_path = "/content/drive/MyDrive/Research_Vaish"
os.makedirs(save_path, exist_ok=True)
save_file = os.path.join(save_path, "best_model_cbam_L3.pth")

# -------------------------------
# Define CBAM Module
# -------------------------------
class ChannelAttention(nn.Module):
    def __init__(self, in_planes, ratio=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)

        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))
        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))
        out = avg_out + max_out
        return self.sigmoid(out)


class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        padding = 3 if kernel_size == 7 else 1
        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avg_out, max_out], dim=1)
        x = self.conv1(x)
        return self.sigmoid(x)


class CBAM(nn.Module):
    def __init__(self, in_planes, ratio=16, kernel_size=7):
        super(CBAM, self).__init__()
        self.ca = ChannelAttention(in_planes, ratio)
        self.sa = SpatialAttention(kernel_size)

    def forward(self, x):
        out = self.ca(x) * x
        out = self.sa(out) * out
        return out

# -------------------------------
# ResNet50 with CBAM on last 3 layers
# -------------------------------
class ResNet50_CBAM_Last3(nn.Module):
    def __init__(self, num_classes=7):
        super(ResNet50_CBAM_Last3, self).__init__()
        base_model = models.resnet50(pretrained=True)

        self.layer0 = nn.Sequential(
            base_model.conv1,
            base_model.bn1,
            base_model.relu,
            base_model.maxpool
        )
        self.layer1 = base_model.layer1
        self.layer2 = base_model.layer2
        self.layer3 = base_model.layer3
        self.layer4 = base_model.layer4

        # Add CBAM on last THREE layers
        self.cbam2 = CBAM(512)
        self.cbam3 = CBAM(1024)
        self.cbam4 = CBAM(2048)

        self.avgpool = base_model.avgpool
        self.fc = nn.Linear(2048, num_classes)

    def forward(self, x):
        x = self.layer0(x)
        x = self.layer1(x)

        x = self.layer2(x)
        x = self.cbam2(x)  # CBAM on layer2

        x = self.layer3(x)
        x = self.cbam3(x)  # CBAM on layer3

        x = self.layer4(x)
        x = self.cbam4(x)  # CBAM on layer4

        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x


# -------------------------------
# Training & Evaluation Setup
# -------------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = ResNet50_CBAM_Last3(num_classes=7).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

# LR schedule: 1e-4 for 50 epochs, then half per 10 epochs
from torch.optim import lr_scheduler
scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[50, 60, 70, 80, 90], gamma=0.5)


def train_one_epoch(model, loader, criterion, optimizer):
    model.train()
    running_loss, correct = 0.0, 0
    for inputs, labels in loader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * inputs.size(0)
        correct += (outputs.argmax(1) == labels).sum().item()
    return running_loss / len(loader.dataset), correct / len(loader.dataset)


def evaluate(model, loader, criterion):
    model.eval()
    running_loss, correct = 0.0, 0
    all_preds, all_labels = [], []
    with torch.no_grad():
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            running_loss += loss.item() * inputs.size(0)
            correct += (outputs.argmax(1) == labels).sum().item()
            all_preds.extend(outputs.argmax(1).cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    return running_loss / len(loader.dataset), correct / len(loader.dataset), all_preds, all_labels

# -------------------------------
# Training Loop with Best Checkpoint
# -------------------------------
best_val_acc = 0.0
save_file = "/content/drive/MyDrive/Research_Vaish/resnet50_cbam_last3.pth"  # path in Google Drive
train_losses, val_losses, train_accs, val_accs = [], [], [], []

for epoch in range(100):
    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)
    val_loss, val_acc, preds, labels = evaluate(model, val_loader, criterion)

    scheduler.step()

    train_losses.append(train_loss)
    val_losses.append(val_loss)
    train_accs.append(train_acc)
    val_accs.append(val_acc)

    print(f"Epoch {epoch+1}/100 | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}")

    # Save best model
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), save_file)
        print(f"âœ… Saved Best Model (Val Acc = {val_acc:.4f})")

# ========================
# Classification Report
# ========================
print("\nClassification Report (Validation Set):")
print(classification_report(targets, preds))

# ========================
# Curves (Loss & Accuracy)
# ========================
plt.plot(train_losses, label="Train Loss")
plt.plot(val_losses, label="Val Loss")
plt.legend()
plt.show()

plt.plot(train_accs, label="Train Acc")
plt.plot(val_accs, label="Val Acc")
plt.legend()
plt.show()

# ========================
# ROC & PR Curves
# ========================
model.load_state_dict(torch.load(save_path))
_, _, preds, targets = evaluate(model, test_loader)

# Convert to one-hot for ROC/PR
targets_np = np.array(targets)
preds_np = np.array(preds)

fpr, tpr, _ = roc_curve(targets_np, preds_np, pos_label=1)
roc_auc = auc(fpr, tpr)

precision, recall, _ = precision_recall_curve(targets_np, preds_np, pos_label=1)

plt.plot(fpr, tpr, label=f"AUC={roc_auc:.2f}")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.show()

plt.plot(recall, precision, label="PR Curve")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.legend()
plt.show()